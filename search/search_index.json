{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Pandaral\u00b7lel A simple and efficient tool to parallelize Pandas operations on all available CPUs. pandarallel is a simple and efficient tool to parallelize Pandas operations on all available CPUs. With a one line code change, it allows any Pandas user to take advandage of his multi-core computer, while pandas uses only one core. pandarallel also offers nice progress bars (available on Notebook and terminal) to get an rough idea of the remaining amount of computation to be done. Without parallelization With parallelization Features pandarallel currently implements the following pandas APIs: Without parallelization With parallelization df.apply(func) df.parallel_apply(func) df.applymap(func) df.parallel_applymap(func) df.groupby(args).apply(func) df.groupby(args).parallel_apply(func) df.groupby(args1).col_name.rolling(args2).apply(func) df.groupby(args1).col_name.rolling(args2).parallel_apply(func) df.groupby(args1).col_name.expanding(args2).apply(func) df.groupby(args1).col_name.expanding(args2).parallel_apply(func) series.map(func) series.parallel_map(func) series.apply(func) series.parallel_apply(func) series.rolling(args).apply(func) series.rolling(args).parallel_apply(func) Requirements On Linux & macOS , no special requirement. On Windows , because of the multiprocessing system (spawn), the function you send to pandarallel must be self contained , and should not depend on external resources. Example: \u2705 Valid on Mac and Linux - \u274c Forbidden On Windows import math def func ( x ): # Here, `math` is defined outside `func`. `func` is not self contained. return math . sin ( x . a ** 2 ) + math . sin ( x . b ** 2 ) \u2705 Valid everywhere def func ( x ): # Here, `math` is defined inside `func`. `func` is self contained. import math return math . sin ( x . a ** 2 ) + math . sin ( x . b ** 2 ) Warning Parallelization has a cost (instantiating new processes, sending data via shared memory, ...), so parallelization is efficient only if the amount of computation to parallelize is high enough. For very little amount of data, using parallelization is not always worth it. Warning Displaying progress bars has a cost and may slighly increase computation time. Examples An example of each API is available here . Benchmark For some examples , here is the comparative benchmark with and without using Pandaral\u00b7lel. Computer used for this benchmark: OS: Linux Ubuntu 16.04 Hardware: Intel Core i7 @ 3.40 GHz - 4 cores For those given examples, parallel operations run approximately 4x faster than the standard operations (except for series.map which runs only 3.2x faster). When should I user pandas , pandarallel or pyspark ? According to pandas documentation : pandas is a fast, powerful, flexible and easy to use open source data analysis and manipulation tool,built on top of the Python programming language. The main pandas drawback is the fact it uses only one core of your computer, even if multiple cores are available. pandarallel gets around this limitation by using all cores of your computer. But, in return, pandarallel need twice the memory that standard pandas operation would normally use. ==> pandarallel should NOT be used if your data cannot fit into memory with pandas itself. In such a case, spark (and its python layer pyspark ) will be suitable. The main drawback of spark is that spark APIs are less convenient to user than pandas APIs (even if this is going better) and also you need a JVM (Java Virtual Machine) on your computer. However, with spark you can: Handle data much bigger than your memory Using a spark cluster, distribute your computation over multiple nodes.","title":"Home"},{"location":"#features","text":"pandarallel currently implements the following pandas APIs: Without parallelization With parallelization df.apply(func) df.parallel_apply(func) df.applymap(func) df.parallel_applymap(func) df.groupby(args).apply(func) df.groupby(args).parallel_apply(func) df.groupby(args1).col_name.rolling(args2).apply(func) df.groupby(args1).col_name.rolling(args2).parallel_apply(func) df.groupby(args1).col_name.expanding(args2).apply(func) df.groupby(args1).col_name.expanding(args2).parallel_apply(func) series.map(func) series.parallel_map(func) series.apply(func) series.parallel_apply(func) series.rolling(args).apply(func) series.rolling(args).parallel_apply(func)","title":"Features"},{"location":"#requirements","text":"On Linux & macOS , no special requirement. On Windows , because of the multiprocessing system (spawn), the function you send to pandarallel must be self contained , and should not depend on external resources. Example: \u2705 Valid on Mac and Linux - \u274c Forbidden On Windows import math def func ( x ): # Here, `math` is defined outside `func`. `func` is not self contained. return math . sin ( x . a ** 2 ) + math . sin ( x . b ** 2 ) \u2705 Valid everywhere def func ( x ): # Here, `math` is defined inside `func`. `func` is self contained. import math return math . sin ( x . a ** 2 ) + math . sin ( x . b ** 2 ) Warning Parallelization has a cost (instantiating new processes, sending data via shared memory, ...), so parallelization is efficient only if the amount of computation to parallelize is high enough. For very little amount of data, using parallelization is not always worth it. Warning Displaying progress bars has a cost and may slighly increase computation time.","title":"Requirements"},{"location":"#examples","text":"An example of each API is available here .","title":"Examples"},{"location":"#benchmark","text":"For some examples , here is the comparative benchmark with and without using Pandaral\u00b7lel. Computer used for this benchmark: OS: Linux Ubuntu 16.04 Hardware: Intel Core i7 @ 3.40 GHz - 4 cores For those given examples, parallel operations run approximately 4x faster than the standard operations (except for series.map which runs only 3.2x faster).","title":"Benchmark"},{"location":"#when-should-i-user-pandas-pandarallel-or-pyspark","text":"According to pandas documentation : pandas is a fast, powerful, flexible and easy to use open source data analysis and manipulation tool,built on top of the Python programming language. The main pandas drawback is the fact it uses only one core of your computer, even if multiple cores are available. pandarallel gets around this limitation by using all cores of your computer. But, in return, pandarallel need twice the memory that standard pandas operation would normally use. ==> pandarallel should NOT be used if your data cannot fit into memory with pandas itself. In such a case, spark (and its python layer pyspark ) will be suitable. The main drawback of spark is that spark APIs are less convenient to user than pandas APIs (even if this is going better) and also you need a JVM (Java Virtual Machine) on your computer. However, with spark you can: Handle data much bigger than your memory Using a spark cluster, distribute your computation over multiple nodes.","title":"When should I user pandas, pandarallel or pyspark?"},{"location":"troubleshooting/","text":"Fail I am on Windows, and pandarallel does not work at all. On Windows, because of the multiprocessing system (spawn), the function you send to pandarallel must be self contained , and should not depend on external resources. Example: \u274c Forbidden import math def func ( x ): # Here, `math` is defined outside `func`. `func` is not self contained. return math . sin ( x . a ** 2 ) + math . sin ( x . b ** 2 ) \u2705 Valid def func ( x ): # Here, `math` is defined inside `func`. `func` is self contained. import math return math . sin ( x . a ** 2 ) + math . sin ( x . b ** 2 ) Fail I have 8 CPUs but pandarallel uses by default only 4 workers, and there is no performance increase (and maybe a little performance decrease) if I manually set the number of workers to a number higher than 4 . pandarallel can only speed up computation until about the number of physical cores your computer has. The majority of recent CPUs (like Intel Core i7) uses hyperthreading. For example, a 4-core hyperthreaded CPU will show 8 CPUs to the operating system, but will really have only 4 physical computation units. You can get the number of cores with import psutil psutil . cpu_count ( logical = False ) Fail I use jupyter lab and instead of progress bars, I see these kind of things: VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=625000), Label(value='0 / 625000') Run the following 3 lines, and you should be able to see the progress bars: pip install ipywidgets jupyter nbextension enable --py widgetsnbextension jupyter labextension install @jupyter-widgets/jupyterlab-manager (You may also have to install nodejs if asked)","title":"Troubleshooting"},{"location":"user_guide/","text":"Installation pip install pandarallel [ --upgrade ] [ --user ] Usage First, you have to import pandarallel : from pandarallel import pandarallel Then, you have to initialize it. pandarallel . initialize () This method takes 5 optional parameters: shm_size_mb : Deprecated - Do not use. nb_workers : Number of workers used for parallelization. (int) If not set, default to the number of cores available. progress_bar : Display progress bars if set to True . (bool, False by default) verbose : The verbosity level (int, 2 by default) 0 - don't display any logs 1 - display only warning logs 2 - display all logs use_memory_fs : (bool, None by default) If set to None and if memory file system is available, pandarallel will use it to transfer data between the main process and workers. If memory file system is not available, pandarallel will default on multiprocessing data transfer (pipe). If set to True , pandarallel will use memory file system to transfer data between the main process and workers and will raise a SystemError if memory file system is not available. If set to False , pandarallel will use multiprocessing data transfer (pipe) to transfer data between the main process and workers. Using memory file system reduces data transfer time between the main process and workers, especially for big data. Memory file system is considered as available only if the directory /dev/shm exists and if the user has read and write rights on it. Basically, memory file system is only available on some Linux distributions (including Ubuntu).","title":"User guide"},{"location":"user_guide/#installation","text":"pip install pandarallel [ --upgrade ] [ --user ]","title":"Installation"},{"location":"user_guide/#usage","text":"First, you have to import pandarallel : from pandarallel import pandarallel Then, you have to initialize it. pandarallel . initialize () This method takes 5 optional parameters: shm_size_mb : Deprecated - Do not use. nb_workers : Number of workers used for parallelization. (int) If not set, default to the number of cores available. progress_bar : Display progress bars if set to True . (bool, False by default) verbose : The verbosity level (int, 2 by default) 0 - don't display any logs 1 - display only warning logs 2 - display all logs use_memory_fs : (bool, None by default) If set to None and if memory file system is available, pandarallel will use it to transfer data between the main process and workers. If memory file system is not available, pandarallel will default on multiprocessing data transfer (pipe). If set to True , pandarallel will use memory file system to transfer data between the main process and workers and will raise a SystemError if memory file system is not available. If set to False , pandarallel will use multiprocessing data transfer (pipe) to transfer data between the main process and workers. Using memory file system reduces data transfer time between the main process and workers, especially for big data. Memory file system is considered as available only if the directory /dev/shm exists and if the user has read and write rights on it. Basically, memory file system is only available on some Linux distributions (including Ubuntu).","title":"Usage"}]}